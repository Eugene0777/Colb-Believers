import json
import time
import logging
import random
import requests
from urllib.parse import urlparse
import os

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s"
)

API_KEY = os.getenv("SOCIALDATA_API_KEY")
COMMUNITY_ID = "1965795131186954572"
HEADERS = {"Authorization": f"Bearer {API_KEY}", "Accept": "application/json"}

ALL_TWEETS_FILE = "all_tweets.json"
LEADERBOARD_FILE = "leaderboard.json"

LINK_TWEETS = [
    "https://twitter.com/jacks12300711/status/1990541572342231158",
    "https://twitter.com/jacks12300711/status/1990711670499385494",
    "https://twitter.com/jacks12300711/status/1991123841674846261",
    "https://twitter.com/jacks12300711/status/1991600494406684714",
    "https://twitter.com/jacks12300711/status/1992326454005428606",
    "https://twitter.com/jacks12300711/status/1993051269750239722",
    "https://twitter.com/jacks12300711/status/1993792499434037318",
    "https://twitter.com/jacks12300711/status/1995083592066793944",
    "https://twitter.com/jacks12300711/status/1996480946216874462",
    "https://twitter.com/jacks12300711/status/1997238765493145988",
]



def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def extract_tweet_id(tweet_url: str) -> str:
    parsed = urlparse(tweet_url)
    parts = parsed.path.strip("/").split("/")
    if "status" in parts:
        idx = parts.index("status")
        return parts[idx + 1].split("?")[0]
    raise ValueError(f"–ù–µ –º–æ–≥—É –∏–∑–≤–ª–µ—á—å ID –∏–∑ URL: {tweet_url}")




def safe_request(url, params=None, retries=8, timeout=30):
    for attempt in range(1, retries + 1):
        try:
            r = requests.get(url, headers=HEADERS, params=params, timeout=timeout)
            r.raise_for_status()
            return r.json()

        except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError):
            logging.warning(f"‚õî NETWORK ISSUE {attempt}/{retries} ‚Äî retrying...")

        except requests.exceptions.RequestException as e:
            logging.warning(f"‚ö† –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ ({attempt}/{retries}): {e}")

        sleep_time = min(12, 2**attempt + random.uniform(0, 1))
        logging.info(f"‚è≥ –ñ–¥—É {sleep_time:.1f} —Å–µ–∫ –∏ –ø–æ–≤—Ç–æ—Ä—è—é...")
        time.sleep(sleep_time)

    raise RuntimeError(f"‚ùå API –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫: {url}")




def extract_best_video(media):
    if media.get("type") not in ("video", "animated_gif"):
        return None

    variants = media.get("video_info", {}).get("variants", [])
    if not variants:
        return None

    video_variants = [
        v for v in variants
        if v.get("content_type", "").startswith("video")
    ]

    if not video_variants:
        return None

    best = max(video_variants, key=lambda v: v.get("bitrate", 0))
    return best.get("url")


def extract_media(tweet):
    media = []

    if "extended_entities" in tweet and "media" in tweet["extended_entities"]:
        media = tweet["extended_entities"]["media"]
    elif "entities" in tweet and "media" in tweet["entities"]:
        media = tweet["entities"]["media"]
    elif "media" in tweet:
        media = tweet["media"]

    unique = []
    seen = set()

    for m in media:
        url = m.get("media_url_https") or m.get("media_url")
        if not url:
            continue

        if url in seen:
            continue  # —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª—è

        seen.add(url)

        unique.append({
            "type": m.get("type"),
            "url": url,
            "thumb": m.get("url"),
            "video_url": extract_best_video(m),
        })

    return unique




def normalize_tweet(tweet: dict) -> dict:
    user = tweet.get("user", {}) or {}
    text = tweet.get("full_text") or tweet.get("text")

    return {
        "created_at": tweet.get("tweet_created_at") or tweet.get("created_at"),
        "id_str": tweet.get("id_str"),
        "text": text,
        "favorite_count": tweet.get("favorite_count", 0),
        "retweet_count": tweet.get("retweet_count", 0),
        "reply_count": tweet.get("reply_count", 0),
        "views_count": tweet.get("views_count", 0),
        "quote_count": tweet.get("quote_count", 0),
        "media": extract_media(tweet),
        "user": {
            "screen_name": user.get("screen_name"),
            "name": user.get("name"),
            "profile_image_url":
                user.get("profile_image_url_https") or user.get("profile_image_url"),
        }
    }




def fetch_community_page(cursor=None, limit=100):
    params = {"type": "Latest", "limit": limit}
    if cursor:
        params["cursor"] = cursor

    url = f"https://api.socialdata.tools/twitter/community/{COMMUNITY_ID}/tweets"
    return safe_request(url, params=params)


def collect_all_community_tweets():
    logging.info("\n=========== –°–ë–û–† –ö–û–ú–¨–Æ–ù–ò–¢–ò ‚Äî –£–ú–ù–´–ô –†–ï–ñ–ò–ú ===========")

    all_tweets = []
    seen = set()
    cursor = None
    page = 0

    while True:
        page += 1
        logging.info(f"\n---- –°–¢–†–ê–ù–ò–¶–ê #{page} ---- cursor={cursor}")

        data = fetch_community_page(cursor)
        tweets = data.get("tweets", [])
        next_cursor = data.get("next_cursor")

        logging.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Ç–≤–∏—Ç–æ–≤: {len(tweets)}, next_cursor={next_cursor}")

        new_count = 0
        for t in tweets:
            tid = t.get("id_str")
            if tid and tid not in seen:
                seen.add(tid)
                all_tweets.append(t)
                new_count += 1

        logging.info(f"‚ûï –Ω–æ–≤—ã—Ö: {new_count} | –≤—Å–µ–≥–æ: {len(all_tweets)}")

        # ================= –ê–ù–¢–ò-–õ–û–ñ–ù–´–ô –ö–û–ù–ï–¶ =================
        if len(tweets) == 0:
            logging.warning("‚ö† –ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ª–æ–∂–Ω—ã–π –ª–∏ —ç—Ç–æ –∫–æ–Ω–µ—Ü...")

            retry_success = False

            # 1) 12 –ø–æ–≤—Ç–æ—Ä–æ–≤
            for attempt in range(1, 13):
                delay = attempt
                logging.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä #{attempt}, –∂–¥—É {delay} —Å–µ–∫...")
                time.sleep(delay)

                retry = fetch_community_page(cursor)
                retry_tweets = retry.get("tweets", [])
                retry_cursor = retry.get("next_cursor")

                if retry_tweets:
                    logging.info("‚úî –õ–û–ñ–ù–´–ô –ö–û–ù–ï–¶: –¥–∞–Ω–Ω—ã–µ –ø–æ—è–≤–∏–ª–∏—Å—å!")

                    added = 0
                    for t in retry_tweets:
                        tid = t.get("id_str")
                        if tid and tid not in seen:
                            seen.add(tid)
                            all_tweets.append(t)
                            added += 1

                    logging.info(f"‚ûï –¥–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ –ø–æ–≤—Ç–æ—Ä–∞: {added}")
                    tweets = retry_tweets
                    next_cursor = retry_cursor
                    retry_success = True
                    break

            # 2) –µ—Å–ª–∏ –≤—Å—ë –ø—É—Å—Ç–æ ‚Äî –ø–∞—É–∑–∞ 3 –º–∏–Ω—É—Ç—ã
            if not retry_success:
                logging.warning("‚è∏ –ü–∞—É–∑–∞ 3 –º–∏–Ω—É—Ç—ã...")
                time.sleep(180)

                retry2 = fetch_community_page(cursor)
                retry2_tweets = retry2.get("tweets", [])
                retry2_cursor = retry2.get("next_cursor")

                if retry2_tweets:
                    logging.info("‚úî –ü–æ—Å–ª–µ –ø–∞—É–∑—ã –¥–∞–Ω–Ω—ã–µ –ø–æ—è–≤–∏–ª–∏—Å—å!")

                    added = 0
                    for t in retry2_tweets:
                        tid = t.get("id_str")
                        if tid and tid not in seen:
                            seen.add(tid)
                            all_tweets.append(t)
                            added += 1

                    tweets = retry2_tweets
                    next_cursor = retry2_cursor
                else:
                    logging.info("üèÅ –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–æ–Ω–µ—Ü ‚Äî –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –¥–∞–∂–µ –ø–æ—Å–ª–µ –ø–∞—É–∑—ã.")
                    break

        if next_cursor is None:
            logging.info("üèÅ next_cursor=None ‚Äî –∫–æ–Ω–µ—Ü –∏—Å—Ç–æ—Ä–∏–∏.")
            break

        cursor = next_cursor
        time.sleep(0.6)

    logging.info(f"\n=== –ì–û–¢–û–í–û: —Å–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤: {len(all_tweets)} ===")
    return all_tweets




def fetch_single_tweet(tweet_id: str):
    url = f"https://api.socialdata.tools/twitter/tweets/{tweet_id}"
    return safe_request(url)


def collect_links_tweets():
    logging.info("\n=========== –°–ë–û–† –¢–í–ò–¢–û–í –ü–û –°–°–´–õ–ö–ê–ú ===========")
    results = []

    for url in LINK_TWEETS:
        logging.info(f"URL: {url}")

        try:
            tid = extract_tweet_id(url)
            raw = fetch_single_tweet(tid)
            results.append(normalize_tweet(raw))
            logging.info(f"‚úì –£—Å–ø–µ—Ö: {tid}")

        except Exception as e:
            logging.error(f"‚úó –û—à–∏–±–∫–∞ –¥–ª—è {url}: {e}")

    return results


# ============================================================
# MERGE
# ============================================================

def merge_tweets(community_raw, link_norm):
    logging.info("\n=========== –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===========")

    final = []
    seen = set()

    for tw in community_raw:
        n = normalize_tweet(tw)
        tid = n.get("id_str")
        if tid and tid not in seen:
            seen.add(tid)
            final.append(n)

    for tw in link_norm:
        tid = tw.get("id_str")
        if tid and tid not in seen:
            seen.add(tid)
            final.append(tw)

    logging.info(f"üî• –ò—Ç–æ–≥–æ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {len(final)} —Ç–≤–∏—Ç–æ–≤")
    return final


# ============================================================
# LEADERBOARD
# ============================================================

def build_leaderboard(tweets):
    board = {}

    for t in tweets:
        user = (t.get("user") or {}).get("screen_name") or "unknown"
        pfp = (t.get("user") or {}).get("profile_image_url")

        stats = board.setdefault(user, {
            "profile_image_url": pfp,
            "posts": 0,
            "likes": 0,
            "retweets": 0,
            "comments": 0,
            "quotes": 0,
            "views": 0,
        })

        stats["posts"] += 1
        stats["likes"] += t.get("favorite_count", 0)
        stats["retweets"] += t.get("retweet_count", 0)
        stats["comments"] += t.get("reply_count", 0)
        stats["quotes"] += t.get("quote_count", 0)
        stats["views"] += t.get("views_count", 0)

    return [[user, stats] for user, stats in board.items()]


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    community_raw = collect_all_community_tweets()
    link_tweets = collect_links_tweets()

    all_tweets = merge_tweets(community_raw, link_tweets)
    save_json(ALL_TWEETS_FILE, all_tweets)
    logging.info(f"üíæ {ALL_TWEETS_FILE} —Å–æ—Ö—Ä–∞–Ω—ë–Ω ({len(all_tweets)} —Ç–≤–∏—Ç–æ–≤)")

    leaderboard = build_leaderboard(all_tweets)
    save_json(LEADERBOARD_FILE, leaderboard)
    logging.info(f"üíæ {LEADERBOARD_FILE} —Å–æ—Ö—Ä–∞–Ω—ë–Ω")



